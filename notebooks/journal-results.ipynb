{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Journal Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author:** Nedal M. Benelmekki\n",
    "- **Date:** 21/10/2025\n",
    "- **Description:** New Journal results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **External Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Add Project Root**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Necessary Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from style import style as st\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **User Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results directory\n",
    "BASE_DIR = Path(\"/home/nedal/Desktop/RFID/ICASSP2026/Bayesian-Enhanced-AoA-Estimator/results/final-trained/final-results\")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"/home/nedal/Desktop/RFID/ICASSP2026/Bayesian-Enhanced-AoA-Estimator/final-results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Regex patterns in .txt files\n",
    "pattern_main = re.compile(r\"MAE:\\s*([\\d\\.]+)\\u00B0\\s*RMSE:\\s*([\\d\\.]+)\\u00B0\")\n",
    "pattern_phys = re.compile(r\"(\\w+): MAE=([\\d\\.]+)\\u00B0, RMSE=([\\d\\.]+)\\u00B0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sweep Files & Collect Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "# Mapping dictionaries for the antenna-array techniques\n",
    "PRIOR_MAP = {\n",
    "    'phase': 'PD',\n",
    "    'weighted': 'WDS',\n",
    "    'ds': 'DS',\n",
    "    'music': 'MUSIC',\n",
    "}\n",
    "METHOD_MAP = {'svi': 'SVI', 'mcmc': 'MCMC'}\n",
    "\n",
    "def map_feature_mode(s: str) -> str:\n",
    "    s_l = s.lower().replace('-', '_').replace(' ', '')\n",
    "    if s_l in ('full', 'full_model', 'fullmodel'):\n",
    "        return 'Full Model'\n",
    "    if s_l in ('width', 'width_model', 'widthmodel'):\n",
    "        return 'Width Model'\n",
    "    if s_l in ('sensor', 'sensor_model', 'sensormodel'):\n",
    "        return 'Sensor Model'\n",
    "    # \"Error\" case\n",
    "    return ' '.join(part.capitalize() for part in s.replace('_', ' ').split())\n",
    "\n",
    "for d in BASE_DIR.iterdir():\n",
    "    if not d.is_dir():\n",
    "        continue  \n",
    "    try:\n",
    "        # Parse name\n",
    "        dir_name = d.name\n",
    "        \n",
    "        # Extract replica\n",
    "        if not dir_name.endswith(('r1', 'r2', 'r3')):\n",
    "            print(f\"[!] Invalid replica format in {dir_name}\")\n",
    "            continue\n",
    "        # Obtain replica and trim\n",
    "        r_str    = dir_name[-2:]\n",
    "        dir_name = dir_name[:-3]\n",
    "        \n",
    "        # Extract percentage value\n",
    "        p_parts = dir_name.split('_p')\n",
    "        if len(p_parts) != 2:\n",
    "            print(f\"[!] Invalid p-value format in {dir_name}\")\n",
    "            continue\n",
    "        dir_name = p_parts[0]\n",
    "        p_str    = 'p' + p_parts[1]\n",
    "        \n",
    "        # Extract method (svi or mcmc)\n",
    "        methods = ['svi', 'mcmc']\n",
    "        method  = None\n",
    "        for m in methods:\n",
    "            if dir_name.endswith(f\"_{m}\"):\n",
    "                method   = m\n",
    "                dir_name = dir_name[:-len(m)-1]\n",
    "                break  \n",
    "        if method is None:\n",
    "            print(f\"[!] No valid method found in {d.name}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract prior observation\n",
    "        priors = ['ds', 'music', 'phase', 'weighted']\n",
    "        prior = None\n",
    "        for p in priors:\n",
    "            if dir_name.startswith(f\"{p}_\"):\n",
    "                prior = p\n",
    "                dir_name = dir_name[len(p)+1:]\n",
    "                break\n",
    "        if prior is None:\n",
    "            print(f\"[!] No valid prior found in {d.name}\")\n",
    "            continue\n",
    "        feature_mode_raw = dir_name\n",
    "        feature_mode_label = map_feature_mode(feature_mode_raw)\n",
    "        \n",
    "        # Validate format\n",
    "        try:\n",
    "            p_val = int(p_str.replace(\"p\", \"\"))\n",
    "            replica = int(r_str.replace(\"r\", \"\"))\n",
    "        except ValueError:\n",
    "            print(f\"[!] Invalid p-value or replica in {d.name}\")\n",
    "            continue\n",
    "            \n",
    "        # Parse model_summary.txt\n",
    "        summary_file = d / \"model_summary.txt\"\n",
    "        if not summary_file.exists():\n",
    "            print(f\"[!] Missing summary in {d}\")\n",
    "            continue\n",
    "\n",
    "        content = summary_file.read_text()\n",
    "\n",
    "        # Extract main performance\n",
    "        match_main = pattern_main.search(content)\n",
    "        if not match_main:\n",
    "            print(f\"[!] No main MAE/RMSE in {d}\")\n",
    "            continue\n",
    "\n",
    "        mae = float(match_main.group(1))\n",
    "        rmse = float(match_main.group(2))\n",
    "\n",
    "        # Extract physics-based comparison\n",
    "        phys_results = dict()\n",
    "        for match in pattern_phys.findall(content):\n",
    "            model, mae_val, rmse_val = match\n",
    "            phys_results[f\"{model.lower()}_mae\"] = float(mae_val)\n",
    "            phys_results[f\"{model.lower()}_rmse\"] = float(rmse_val)\n",
    "\n",
    "        # Add record with desired labels\n",
    "        records.append({\n",
    "            \"prior\": PRIOR_MAP.get(prior, prior.upper()),\n",
    "            \"features\": feature_mode_label,\n",
    "            \"method\": METHOD_MAP.get(method, method.upper()),\n",
    "            \"samples_%\": p_val,\n",
    "            \"replica\": replica,\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse,\n",
    "            **phys_results\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[x] Error parsing {d}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "print(f\"[i] Loaded {len(df)} entries\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocesing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Average Across Replicas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"prior\", \"features\", \"method\", \"samples_%\"]\n",
    "df_avg     = df.groupby(group_cols).agg({\"mae\": \"mean\", \"rmse\": \"mean\"}).reset_index()\n",
    "print(\"\\n[i] Averaged across replicas:\")\n",
    "print(df_avg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for all configurations\n",
    "print(\"\\n[i] Table for publication (sorted by MAE):\")\n",
    "styled_table = df_avg.sort_values(\"mae\").round(4)\n",
    "print(styled_table.to_string(index=False))\n",
    "\n",
    "# Export to CSV for easy import into LaTeX or other tools\n",
    "output_csv = OUTPUT_DIR / \"results_table.csv\"\n",
    "styled_table.to_csv(output_csv, index=False)\n",
    "print(f\"\\n[i] Exported table to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Data vs. Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Plot lines for each configuration\n",
    "for (prior, method, features), subset in df_avg.groupby([\"prior\", \"method\", \"features\"]):\n",
    "    subset = subset.sort_values(\"samples_%\")\n",
    "    plt.plot(\n",
    "        subset[\"samples_%\"], \n",
    "        subset[\"mae\"], \n",
    "        label=f\"{prior}-{method}-{features}\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Training Data (\\%)\")\n",
    "plt.ylabel(\"Mean Absolute Error ($^\\circ$)\")\n",
    "plt.title(\"Performance vs. Training Data Size\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "output_fig = OUTPUT_DIR / \"performance_vs_samples.png\"\n",
    "plt.savefig(output_fig, dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Best Configuration Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compute Best Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and save the best configuration for each sample size\n",
    "best_by_sample = df_avg.loc[df_avg.groupby(\"samples_%\")[\"mae\"].idxmin()]\n",
    "print(\"\\n[i] Best configuration for each sample size:\")\n",
    "print(best_by_sample[[\"samples_%\", \"prior\", \"features\", \"method\", \"mae\", \"rmse\"]])\n",
    "best_sample_output = OUTPUT_DIR / \"best_by_sample_size.csv\"\n",
    "best_by_sample.to_csv(best_sample_output, index=False)\n",
    "print(f\"[i] Saved best configurations by sample size to {best_sample_output}\")\n",
    "\n",
    "# Find overall best configuration\n",
    "best_idx = df_avg[\"mae\"].idxmin()\n",
    "best_config = df_avg.loc[best_idx].to_dict()\n",
    "print(f\"\\n[i] Best overall configuration: {best_config['prior']}-{best_config['features']}-{best_config['method']} with {best_config['samples_%']}% samples\")\n",
    "\n",
    "# Get the full data for the best configuration\n",
    "best_data = df[(df[\"prior\"] == best_config[\"prior\"]) & \n",
    "              (df[\"features\"] == best_config[\"features\"]) & \n",
    "              (df[\"method\"] == best_config[\"method\"]) & \n",
    "              (df[\"samples_%\"] == best_config[\"samples_%\"])]\n",
    "\n",
    "# Compare with baseline methods\n",
    "if len([col for col in best_data.columns if col.endswith('_mae')]) > 0:\n",
    "    baselines = [\"music_mae\", \"ds_mae\", \"weighted_mae\", \"phase_mae\"]\n",
    "    baseline_names = [\"MUSIC\", \"DS\", \"Weighted DS\", \"Phase\"]\n",
    "    \n",
    "    improvements = []\n",
    "    for baseline in baselines:\n",
    "        if baseline in best_data.columns:\n",
    "            avg_baseline = best_data[baseline].mean()\n",
    "            improvement = (avg_baseline - best_data[\"mae\"].mean()) / avg_baseline * 100\n",
    "            improvements.append((baseline_names[baselines.index(baseline)], avg_baseline, best_data[\"mae\"].mean(), improvement))\n",
    "    \n",
    "    print(\"\\n[i] Improvement over baselines:\")\n",
    "    for name, baseline, bayesian, improvement in improvements:\n",
    "        print(f\"{name}: {baseline:.4f} deg ? {bayesian:.4f} deg ({improvement:.2f}% improvement)\")\n",
    "        \n",
    "    # Save improvement stats\n",
    "    improvement_output = OUTPUT_DIR / \"improvement_summary.txt\"\n",
    "    with open(improvement_output, \"w\") as f:\n",
    "        f.write(f\"Best configuration: {best_config['prior']}-{best_config['features']}-{best_config['method']} with {best_config['samples_%']}% samples\\n\")\n",
    "        f.write(f\"MAE: {best_config['mae']:.4f} deg, RMSE: {best_config['rmse']:.4f} deg\\n\\n\")\n",
    "        f.write(\"Improvement over baselines:\\n\")\n",
    "        for name, baseline, bayesian, improvement in improvements:\n",
    "            f.write(f\"{name}: {baseline:.4f} deg ? {bayesian:.4f} deg ({improvement:.2f}% improvement)\\n\")\n",
    "    print(f\"[i] Saved improvement summary to {improvement_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len([col for col in best_data.columns if col.endswith('_mae')]) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    methods = [\"Bayesian\"] + [name for name, _, _, _ in improvements]\n",
    "    errors = [best_data[\"mae\"].mean()] + [baseline for _, baseline, _, _ in improvements]\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = plt.bar(methods, errors)\n",
    "    bars[0].set_color('green')\n",
    "    \n",
    "    plt.ylabel(\"Mean Absolute Error ($^\\circ$)\")\n",
    "    plt.title(f\"Performance Comparison: {best_config['prior']}-{best_config['features']}-{best_config['method']} vs. Baselines\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add error values on top of bars\n",
    "    for i, v in enumerate(errors):\n",
    "        plt.text(i, v + 0.1, f\"{v:.2f} $^\\circ$\", ha='center')\n",
    "    \n",
    "    # Save figure\n",
    "    comparison_fig = OUTPUT_DIR / \"baseline_comparison.png\"\n",
    "    plt.savefig(comparison_fig, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[i] Saved baseline comparison chart to {comparison_fig}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Top Perormance Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to avoid geometric model\n",
    "print(df_avg[\"features\"].unique())\n",
    "df_width = df_avg[df_avg[\"features\"] == \"Width Only\"].copy()\n",
    "if df_width.empty:\n",
    "    raise ValueError('No rows with features == \"Width Only\". Check your mapping upstream.')\n",
    "\n",
    "# Top configurations\n",
    "top_configs = (\n",
    "    df_width.groupby([\"prior\", \"method\"])[\"mae\"]\n",
    "    .mean()\n",
    "    .nsmallest(1)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "top_configs_list = list(zip(top_configs[\"prior\"], top_configs[\"method\"]))\n",
    "\n",
    "print(\"\\n[i] Top performing configurations:\")\n",
    "for i, (prior, method) in enumerate(top_configs_list, 1):\n",
    "    print(f\"{i}. {prior} - Width Only - {method}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors  = [\"red\", \"#E69F00\", \"#009E73\", \"#D55E00\", \"#CC79A7\", \"#56B4E9\"]\n",
    "markers = [\"o\", \"s\", \"D\", \"^\", \"v\", \"P\", \"X\", \"*\"]\n",
    "\n",
    "for i, (prior, method) in enumerate(top_configs_list):\n",
    "    subset = df_width[(df_width[\"prior\"] == prior) & \n",
    "                  (df_width[\"method\"] == method)]\n",
    "    subset = subset.sort_values(\"samples_%\")\n",
    "    plt.plot(subset[\"samples_%\"], subset[\"mae\"], linewidth = 4.0, label=f\"{prior} - Width Only - {method}\", color=colors[i % len(colors)],\n",
    "        marker=markers[i % len(markers)], markersize=7,)\n",
    "\n",
    "plt.xlabel(\"Training Data (\\%)\", fontsize=18, labelpad=20)\n",
    "plt.ylabel(\"Mean Absolute Error ($^\\circ$)\", fontsize=18, labelpad=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title(\"Performance vs. Training Data Size\", fontsize=20)\n",
    "plt.legend(loc='best', fontsize = 16)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "top_perf_fig = OUTPUT_DIR / \"top_performers_vs_samples.png\"\n",
    "plt.savefig(top_perf_fig, dpi=600)\n",
    "plt.show()\n",
    "print(f\"[i] Saved top performers chart to {top_perf_fig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prior Type Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_performance = df_avg.groupby(\"prior\")[\"mae\"].mean().reset_index()\n",
    "prior_performance = prior_performance.sort_values(\"mae\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(prior_performance[\"prior\"], prior_performance[\"mae\"])\n",
    "plt.ylabel(\"Mean Absolute Error ($^\\circ$)\")\n",
    "plt.title(\"Performance by Prior Type\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "for i, v in enumerate(prior_performance[\"mae\"]):\n",
    "    plt.text(i, v + 0.02, f\"{v:.2f} deg\", ha='center')\n",
    "\n",
    "prior_fig = OUTPUT_DIR / \"prior_performance.png\"\n",
    "plt.savefig(prior_fig, dpi=600)\n",
    "plt.show()\n",
    "print(f\"[i] Saved prior performance chart to {prior_fig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sample Efficiency Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_overall_config = best_config[\"prior\"], best_config[\"method\"], best_config[\"features\"]\n",
    "\n",
    "full_perf = df_avg[(df_avg[\"prior\"] == best_config[\"prior\"]) & \n",
    "                 (df_avg[\"method\"] == best_config[\"method\"]) & \n",
    "                 (df_avg[\"features\"] == best_config[\"features\"]) &\n",
    "                 (df_avg[\"samples_%\"] == 100)][\"mae\"].values\n",
    "\n",
    "if len(full_perf) > 0:\n",
    "    full_perf = full_perf[0]\n",
    "    \n",
    "    sample_perf = df_avg[(df_avg[\"prior\"] == best_config[\"prior\"]) & \n",
    "                        (df_avg[\"method\"] == best_config[\"method\"]) & \n",
    "                        (df_avg[\"features\"] == best_config[\"features\"])]\n",
    "    \n",
    "    sample_perf = sample_perf.sort_values(\"samples_%\")\n",
    "    sample_perf[\"relative_perf\"] = full_perf / sample_perf[\"mae\"] * 100\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(sample_perf[\"samples_%\"], sample_perf[\"relative_perf\"], \n",
    "             marker='o', linewidth=2)\n",
    "    plt.axhline(y=100, color='gray', linestyle='--')\n",
    "    \n",
    "    plt.xlabel(\"Training Data (\\%)\")\n",
    "    plt.ylabel(\"Performance Relative to Full Dataset (\\%)\")\n",
    "    plt.title(f\"Sample Efficiency: {best_config['prior']}-{best_config['features']}-{best_config['method']}\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    efficiency_fig = OUTPUT_DIR / \"sample_efficiency.png\"\n",
    "    plt.savefig(efficiency_fig, dpi=600)\n",
    "    plt.show()\n",
    "    print(f\"[i] Saved sample efficiency chart to {efficiency_fig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Method Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SVI vs MCMC performance\n",
    "method_comparison = df_avg.groupby([\"method\", \"features\"])[\"mae\"].mean().reset_index()\n",
    "method_pivot = method_comparison.pivot(index=\"features\", columns=\"method\", values=\"mae\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = method_pivot.plot(kind=\"bar\", figsize=(10, 6), color=[\"#0A2342\", \"#17BECF\"])\n",
    "ax.set_ylabel(\"MAE ($^\\circ$)\", labelpad=20)\n",
    "ax.set_xlabel(\"Feature Set\", labelpad=20)\n",
    "ax.set_title(\"SVI vs MCMC Performance by Feature Set\")\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, ha='center')\n",
    "ax.legend(title=None)\n",
    "\n",
    "plt.subplots_adjust(bottom=0.5)\n",
    "\n",
    "method_fig = OUTPUT_DIR / \"svi_vs_mcmc.png\"\n",
    "plt.savefig(method_fig, dpi=600)\n",
    "plt.show()\n",
    "print(f\"[i] Saved method comparison chart to {method_fig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Relative Improvement Heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_ref = \"phase_mae\"\n",
    "df_improv = df.copy()\n",
    "df_improv[\"improvement_%\"] = (\n",
    "    (df_improv[baseline_ref] - df_improv[\"mae\"]) / df_improv[baseline_ref] * 100\n",
    ")\n",
    "\n",
    "avg_improv = df_improv.groupby([\"prior\", \"method\", \"features\"])[\"improvement_%\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "pivot_improv = avg_improv.pivot_table(\n",
    "    values=\"improvement_%\",\n",
    "    index=\"prior\",\n",
    "    columns=[\"method\", \"features\"]\n",
    ")\n",
    "sns.heatmap(pivot_improv, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", cbar_kws={'label': '% Improvement'})\n",
    "plt.title(f\"Average Improvement over {baseline_ref.split('_')[0].upper()} Baseline\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"improvement_heatmap.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prior vs. Posterior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for features\n",
    "FEATURE_LABELS = {\n",
    "    \"phase1_mean\":        r\"Phase 1 ($\\varphi_1$)\",\n",
    "    \"phase2_mean\":        r\"Phase 2 ($\\varphi_2$)\",\n",
    "    \"phase_diff\":         r\"Phase Diff. ($\\Delta \\varphi$)\",\n",
    "    \"mag1_mean\":          r\"Mag. 1 ($mag_1$)\",\n",
    "    \"mag2_mean\":          r\"Mag. 2 ($mag_2$)\",\n",
    "    \"rssi1_mean\":         r\"RSSI$_1$\",\n",
    "    \"rssi2_mean\":         r\"RSSI$_2$\",\n",
    "    \"rssi_diff\":          r\"$\\Delta$ RSSI \",\n",
    "    \"phasor1_real_mean\":  r\"$\\Re\\{z_1\\}$\",\n",
    "    \"phasor1_imag_mean\":  r\"$\\Im\\{z_1\\}$\",\n",
    "    \"phasor2_real_mean\":  r\"$\\Re\\{z_2\\}$\",\n",
    "    \"phasor2_imag_mean\":  r\"$\\Im\\{z_2\\}$\",\n",
    "    \"wavelength\":         r\"Wavelength ($\\lambda$)\",\n",
    "    \"distance\":           r\"Distance ($D$)\",\n",
    "    \"width\":              r\"Width ($W$)\",\n",
    "    \"bias\":               r\"Bias\"\n",
    "}\n",
    "\n",
    "def _label_for(name: str) -> str:\n",
    "    return FEATURE_LABELS.get(name, name.replace('_', ' ').title())\n",
    "\n",
    "def _normal_pdf(x, mu, sd):\n",
    "    sd = max(float(sd), 1e-12)\n",
    "    return (1.0 / (sd * np.sqrt(2.0 * np.pi))) * np.exp(-0.5 * ((x - mu) / sd) ** 2)\n",
    "\n",
    "# Parse summary file\n",
    "_SUMMARY_RE = re.compile(r\"^\\s*([A-Za-z0-9_]+)\\s*:\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*±\\s*([+-]?\\d+(?:\\.\\d+)?)\\s*$\")\n",
    "\n",
    "def load_posteriors_from_summary(summary_path: str):\n",
    "    \"\"\"\n",
    "    Returns a dict: name -> (mean, std) where std is 1σ (converted from '± 2σ')\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    with open(summary_path, 'r') as f:\n",
    "        for line in f:\n",
    "            m = _SUMMARY_RE.match(line)\n",
    "            if m:\n",
    "                name = m.group(1)\n",
    "                mean = float(m.group(2))\n",
    "                two_sigma = float(m.group(3))\n",
    "                std = two_sigma / 2.0\n",
    "                out[name] = (mean, std)\n",
    "    return out\n",
    "\n",
    "# Plot prior vs posterior for selected params (weights & the bias)\n",
    "def plot_param_priors_vs_posteriors(\n",
    "    summary_path: str,\n",
    "    out_path: str,\n",
    "    w_prior_sd: float = 1.0,\n",
    "    b_prior_sd: float = 5.0,\n",
    "    params: list[str] | None = None,\n",
    "    show_true_lines: bool = False,\n",
    "    true_vals: dict[str, float] | None = None,\n",
    "    tight_margin: float = 4.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads posterior means/stds from your summary and overlays the Normal priors.\n",
    "\n",
    "    - summary_path: path to model_summary.txt\n",
    "    - out_path:     where to save the figure (png, pdf, etc.)\n",
    "    - w_prior_sd:   σ_w from your HBLRConfig (for all weights)\n",
    "    - b_prior_sd:   σ_b from your HBLRConfig (for bias)\n",
    "    - params:       optional subset of parameter names to plot (order preserved)\n",
    "    - show_true_lines/true_vals: optional vertical black lines if you have known truths\n",
    "    - tight_margin: x-range ~ +/- tight_margin * max(prior_sd, post_sd) around means\n",
    "    \"\"\"\n",
    "    posts = load_posteriors_from_summary(summary_path)\n",
    "    if not posts:\n",
    "        raise ValueError(f\"No parameters parsed from {summary_path}\")\n",
    "\n",
    "    keys = list(posts.keys())\n",
    "    if params:\n",
    "        keys = [k for k in params if k in posts]\n",
    "    else:\n",
    "        if \"Bias\" in keys:\n",
    "            keys = [k for k in keys if k != \"Bias\"] + [\"Bias\"]\n",
    "\n",
    "    n = len(keys)\n",
    "    ncols = min(4, max(1, int(np.ceil(np.sqrt(n)))))\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(4*ncols + 2, 3*nrows + 1))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "    for i, name in enumerate(keys):\n",
    "        ax = axes[i]\n",
    "        mean, post_sd = posts[name]\n",
    "        prior_sd = b_prior_sd if name.lower() == \"bias\" else w_prior_sd\n",
    "        prior_mu = 0.0\n",
    "\n",
    "        span = tight_margin * max(prior_sd, post_sd)\n",
    "        center = 0.5 * (prior_mu + mean)\n",
    "        x_min = min(prior_mu, mean) - span\n",
    "        x_max = max(prior_mu, mean) + span\n",
    "        if x_max - x_min < 1e-6:\n",
    "            x_min, x_max = center - span, center + span\n",
    "\n",
    "        x = np.linspace(x_min, x_max, 800)\n",
    "\n",
    "        # Prior: red dashed + fill\n",
    "        prior_pdf = _normal_pdf(x, prior_mu, prior_sd)\n",
    "        ax.plot(x, prior_pdf, 'r--', lw=2, label='Prior')\n",
    "        ax.fill_between(x, 0, prior_pdf, color='red', alpha=0.2)\n",
    "\n",
    "        # Posterior: blue solid + fill\n",
    "        post_pdf = _normal_pdf(x, mean, post_sd)\n",
    "        ax.plot(x, post_pdf, 'b-', lw=2, label='Posterior')\n",
    "        ax.fill_between(x, 0, post_pdf, color='blue', alpha=0.2)\n",
    "\n",
    "        # “True” vertical line\n",
    "        if show_true_lines and true_vals and name in true_vals:\n",
    "            ax.axvline(true_vals[name], color='k', lw=2, label='True')\n",
    "\n",
    "        ax.set_title(_label_for(name))\n",
    "        ax.set_xlabel('Weight Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"upper left\")\n",
    "\n",
    "    # Hide any unused axes\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "summary_path = \"/home/nedal/Desktop/RFID/ICASSP2026/Bayesian-Enhanced-AoA-Estimator/results/final-trained/final-results/music_full_mcmc_p5_r1/weights_bias_summary.txt\"  # adjust path\n",
    "out_path     = \"/home/nedal/Desktop/RFID/ICASSP2026/Bayesian-Enhanced-AoA-Estimator/final-results/prior_vs_posterior_weights.png\"\n",
    "\n",
    "# Option A: plot all parsed parameters (weights + bias)\n",
    "plot_param_priors_vs_posteriors(\n",
    "    summary_path=summary_path,\n",
    "    out_path=out_path,\n",
    "    w_prior_sd=1.0,\n",
    "    b_prior_sd=5.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
